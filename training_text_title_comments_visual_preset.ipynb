{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from utils.models.modelUtils import loadMeanFromFile, calcAccConcatModel\n",
    "\n",
    "from final_models import create_image_inceptionv3_model, create_text_model, create_model_meta, buildConcatModelTitleCommentsVisual\n",
    "\n",
    "from utils.callbacks.MyCallbacks import MyCallbacks\n",
    "from utils.datagenUtils.three_modal.DataSeqTitleCommentsVisual import DataSequenceTitleCommentsVisual\n",
    "from utils.datagenUtils.datagenUtils import checkDataframe\n",
    "\n",
    "from utils.telegramUtils.telegram_bot import telegram_send_message\n",
    "from utils.textUtils.textpreprocessing import FakeDetectionDataTrainVal, FakeDetectionDataTest\n",
    "from utils.textUtils.commentsProcessing import FakeDetectionDataCommentsTest, FakeDetectionDataCommentsTrainVal\n",
    "\n",
    "from utils.callbacks.callbackUtils import plotTimesPerEpoch\n",
    "\n",
    "from utils.fileDirUtils.fileDirUtils import createDirIfNotExists, writeLog\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Batch Size: 96, Epochs: 10, Optimizer: Adam, Learning Rate; 1e-05, Beta_1: 0.9, Beta_2: 0.999, Epsilon: 1e-08, Image Sizes: (768, 768)\n"
     ]
    }
   ],
   "source": [
    "#Verbose settings:\n",
    "verbose = False\n",
    "TF_VERBOSE = 1 # 1 = Progress bar 2 = one line per epoch only!\n",
    "TF_DETERMINISTIC_OPS = 1 # Makes everything also on GPU deterministic\n",
    "\n",
    "# Classes:\n",
    "NUM_CLASS = 2  # FAKE | NO FAKE\n",
    "\n",
    "# Hyperparameters\n",
    "GLOBAL_BATCH_SIZE = 96\n",
    "EPOCHS = 10\n",
    "\n",
    "# Optimizer parameters:\n",
    "# Adam\n",
    "LEARNING_RATE = 1e-5\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "#optimizers:\n",
    "\n",
    "optimizer = Adam(LEARNING_RATE, BETA_1, BETA_2, EPSILON)\n",
    "\n",
    "# Bert Parameters\n",
    "MAX_SEQUENCE_LENGTH = 128 # from model definition!\n",
    "\n",
    "# Image Model  Parameters\n",
    "IMG_WIDTH = 768\n",
    "IMG_HEIGHT = 768\n",
    "IMG_DEPTH = 3\n",
    "IMG_SIZES = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Training switches\n",
    "RETRAIN_WHOLE_MODEL = False\n",
    "loadPretrained = False # Attention that model weights path is set !\n",
    "\n",
    "# Custom telegram send text \n",
    "CUSTOM_TEXT = f'Batch Size: {GLOBAL_BATCH_SIZE}, Epochs: {EPOCHS}, Optimizer: Adam, Learning Rate; {LEARNING_RATE}, Beta_1: {BETA_1}, Beta_2: {BETA_2}, Epsilon: {EPSILON}, Image Sizes: {IMG_SIZES}'\n",
    "\n",
    "\n",
    "telegram_send_message(f'-----------------START-----------------')\n",
    "print('START')\n",
    "print(CUSTOM_TEXT)\n",
    "telegram_send_message(CUSTOM_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToRootModelDir = '/home/armin/repos/fkd-model-handling/models/best_models'\n",
    "\n",
    "pathToBertModelTitle = os.path.join(pathToRootModelDir, 'single_text_title', 'weights-improvement-02-0.88.hdf5')\n",
    "pathToBertModelComments = os.path.join(pathToRootModelDir, 'single_text_comments', 'weights-improvement-03-0.87.hdf5')\n",
    "pathToImageModel = os.path.join(pathToRootModelDir, 'single_visual', 'weights-improvement-02-0.81.hdf5')\n",
    "pathToMetaModel = os.path.join(pathToRootModelDir, 'single_meta', 'weights-improvement-100-0.62.hdf5')\n",
    "\n",
    "pathToImagesTrain = '/home/armin/repos/FKD-Dataset/006_images_resized_2/train/' \n",
    "pathToCSVWithFileNamesAndLabelsTrain = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/train_text_image_meta_label.csv'\n",
    "\n",
    "pathToImagesVal = '/home/armin/repos/FKD-Dataset/006_images_resized_2/val/' \n",
    "pathToCSVWithFileNamesAndLabelsVal = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/val_text_image_meta_label.csv'\n",
    "\n",
    "pathToImagesTest = '/home/armin/repos/FKD-Dataset/006_images_resized_2/test/' \n",
    "pathToCSVWithFileNamesAndLabelsTest = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/test_text_image_meta_label.csv'\n",
    "\n",
    "checkpointDir = '/home/armin/repos/FKD-Dataset/011_checkpoints'\n",
    "\n",
    "pathToMeans = '/home/armin/repos/FKD-Dataset/010_configs/means_resized_768.txt'\n",
    "\n",
    "titleCommentsVisualMetaModelWeightsPath = \"/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_2020-07-18_20:37/weights-improvement-05-0.94.hdf5\"\n",
    "\n",
    "root = '/home/armin/repos/fkd-model-handling/'\n",
    "bert_model_dir = os.path.join(root, 'multi_cased_L-12_H-768_A-12')\n",
    "bert_config_file = os.path.join(bert_model_dir, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(bert_model_dir, \"bert_model.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "\n",
    "# Time settings:\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "#Checkpoint settings:\n",
    "checkpoint_name = \"model_concat_title_comments_visual\"\n",
    "\n",
    "checkpointDir = os.path.join(checkpointDir, (checkpoint_name + '_' + current_time))\n",
    "\n",
    "fileName=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filePath = os.path.join(checkpointDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_2020-07-28_22:06/tensorboard does not exist, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# Callback Handling:\n",
    "tensorboardDir = os.path.join(checkpointDir, 'tensorboard')\n",
    "\n",
    "createDirIfNotExists(tensorboardDir)\n",
    "createDirIfNotExists(checkpointDir)\n",
    "\n",
    "\n",
    "callbacks_list = MyCallbacks(tensorboardDir, filePath, earlyStopping=True).createCheckpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFilePath = os.path.join(checkpointDir, \"logs.txt\")\n",
    "writeLog(logFilePath, CUSTOM_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Needed to remove 78 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 28 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 10 elements to make it matching from dataframe\n"
     ]
    }
   ],
   "source": [
    "df_train_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTrain, header=0, sep='\\t')\n",
    "df_test_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTest, header=0, sep='\\t')\n",
    "df_val_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsVal, header=0, sep='\\t')\n",
    "\n",
    "df_train_['2_way_label'] = df_train_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_test_['2_way_label'] = df_test_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_val_['2_way_label'] = df_val_['2_way_label'].apply(lambda x: np.array(x))\n",
    "\n",
    "df_train = df_train_\n",
    "df_test = df_test_\n",
    "df_val = df_val_\n",
    "\n",
    "# df_train = df_train[:500]\n",
    "# df_test = df_test[:500]\n",
    "# df_val = df_val[:500]\n",
    "\n",
    "df_train = checkDataframe(df_train, GLOBAL_BATCH_SIZE)\n",
    "df_val = checkDataframe(df_val, GLOBAL_BATCH_SIZE)\n",
    "df_test = checkDataframe(df_test, GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560544it [02:08, 4372.81it/s]\n",
      "58944it [00:13, 4311.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58944it [00:13, 4325.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560544it [32:01, 291.71it/s]\n",
      "58944it [03:25, 286.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 55475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58944it [03:25, 287.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 39424\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_model_dir, \"vocab.txt\"))\n",
    "\n",
    "all_title_data = FakeDetectionDataTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "all_title_test = FakeDetectionDataTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "comments_data_train = FakeDetectionDataCommentsTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "comments_data_test = FakeDetectionDataCommentsTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "train_title_x = all_title_data.train_x\n",
    "train_title_val_x = all_title_data.val_x\n",
    "test_title_x = all_title_test.test_x\n",
    "\n",
    "comments_train_x = comments_data_train.train_x\n",
    "comments_val_x = comments_data_train.val_x\n",
    "comments_test_x = comments_data_test.test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansOfDataset = loadMeanFromFile(pathToMeans, verbose)\n",
    "\n",
    "train_seq = DataSequenceTitleCommentsVisual(df_train, pathToImagesTrain, train_title_x, comments_train_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "test_seq = DataSequenceTitleCommentsVisual(df_test, pathToImagesTest, test_title_x, comments_test_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "val_seq = DataSequenceTitleCommentsVisual(df_val,pathToImagesVal, train_title_val_x, comments_val_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "STEP_SIZE_TRAIN = len(df_train) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_TEST = len(df_test) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_VAL = len(df_val) // GLOBAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.io.gfile import GFile\n",
    "\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "# from tensorflow.keras.applications.resnet_v2 import ResNet101V2\n",
    "\n",
    "# from tensorflow.keras.layers import Concatenate, Dense, Dropout, GlobalAveragePooling2D, Input, Lambda\n",
    "# from tensorflow.keras import Model\n",
    "\n",
    "# from tensorflow.keras.initializers import GlorotNormal\n",
    "# from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "\n",
    "# def buildConcatModelTitleCommentsVisual(model_bert_title, model_bert_comments, model_image, NUM_CLASS):\n",
    "#     model_bert_title_output = model_bert_title.get_layer('bert_output_layer_768_title').output\n",
    "#     model_bert_comments_output = model_bert_comments.get_layer('bert_output_layer_768').output\n",
    "#     model_image_output = model_image.get_layer('img_dense_768').output\n",
    "    \n",
    "#     # Build new models, without softmax\n",
    "#     model_bert_title = Model(model_bert_title.inputs, model_bert_title_output)    \n",
    "#     model_bert_comments = Model(model_bert_comments.inputs, model_bert_comments_output)\n",
    "#     model_image = Model(model_image.inputs, model_image_output)    \n",
    "    \n",
    "#     # Build multimodal model\n",
    "    \n",
    "#     concatenate = Concatenate()([model_bert_title.output, model_bert_comments.output, model_image_output]) # Fusion\n",
    "#     x = Dense(1024, activation = 'relu')(concatenate)\n",
    "#     x = Dropout(0.4)(x)\n",
    "#     x = Dense(256, activation = 'relu')(x) \n",
    "#     output = Dense(NUM_CLASS, activation='softmax', name='final_multimodal_output')(x)\n",
    "#     model_concat = Model([model_image.input, model_bert_title.input, model_bert_comments.input], output)\n",
    "#     return model_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "bert shape (None, 128, 768)\n",
      "bert shape (None, 128, 768)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9608 - loss: 0.1003\n",
      "Epoch 00001: val_loss improved from inf to 0.13903, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_2020-07-28_22:06/weights-improvement-01-0.95.hdf5\n",
      "5839/5839 [==============================] - 6355s 1s/step - accuracy: 0.9608 - loss: 0.1003 - val_accuracy: 0.9480 - val_loss: 0.1390 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9650 - loss: 0.0896\n",
      "Epoch 00002: val_loss did not improve from 0.13903\n",
      "5839/5839 [==============================] - 6308s 1s/step - accuracy: 0.9650 - loss: 0.0896 - val_accuracy: 0.9477 - val_loss: 0.1407 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9654 - loss: 0.0878\n",
      "Epoch 00003: val_loss improved from 0.13903 to 0.13631, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_2020-07-28_22:06/weights-improvement-03-0.95.hdf5\n",
      "5839/5839 [==============================] - 6332s 1s/step - accuracy: 0.9654 - loss: 0.0878 - val_accuracy: 0.9485 - val_loss: 0.1363 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9657 - loss: 0.0870\n",
      "Epoch 00004: val_loss improved from 0.13631 to 0.13549, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_2020-07-28_22:06/weights-improvement-04-0.95.hdf5\n",
      "5839/5839 [==============================] - 6324s 1s/step - accuracy: 0.9657 - loss: 0.0870 - val_accuracy: 0.9495 - val_loss: 0.1355 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9661 - loss: 0.0862\n",
      "Epoch 00005: val_loss did not improve from 0.13549\n",
      "5839/5839 [==============================] - 6307s 1s/step - accuracy: 0.9661 - loss: 0.0862 - val_accuracy: 0.9487 - val_loss: 0.1360 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9663 - loss: 0.0857\n",
      "Epoch 00006: val_loss improved from 0.13549 to 0.13367, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_2020-07-28_22:06/weights-improvement-06-0.95.hdf5\n",
      "5839/5839 [==============================] - 6328s 1s/step - accuracy: 0.9663 - loss: 0.0857 - val_accuracy: 0.9490 - val_loss: 0.1337 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9666 - loss: 0.0853\n",
      "Epoch 00007: val_loss did not improve from 0.13367\n",
      "5839/5839 [==============================] - 6510s 1s/step - accuracy: 0.9666 - loss: 0.0853 - val_accuracy: 0.9493 - val_loss: 0.1361 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.9667 - loss: 0.0849\n",
      "Epoch 00008: val_loss did not improve from 0.13367\n",
      "5839/5839 [==============================] - 6500s 1s/step - accuracy: 0.9667 - loss: 0.0849 - val_accuracy: 0.9492 - val_loss: 0.1339 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "1169/5839 [=====>........................] - ETA: 1:17:51 - accuracy: 0.9665 - loss: 0.0861"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # Preparing:\n",
    "    model_bert_title, model_bert_title_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModelTitle, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "    model_bert_comments, model_bert_comments_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModelComments, isTrainable=RETRAIN_WHOLE_MODEL) \n",
    "    model_image, model_image_custom_layer = create_image_inceptionv3_model(NUM_CLASS=NUM_CLASS,isPreTrained=True,pathToInceptionV3ModelWeights=pathToImageModel, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Handling same layer name error:\n",
    "    for i, layer in enumerate(model_bert_title.layers):\n",
    "        layer._name = layer._name + '_title' # Consider the _ for the setter\n",
    "    \n",
    "    model_concat = buildConcatModelTitleCommentsVisual(model_bert_title, model_bert_comments, model_image, 2)\n",
    "    \n",
    "    if loadPretrained:\n",
    "        model_concat.load_weights(titleCommentsVisualMetaModelWeightsPath)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------Text Title MODEL --------------')\n",
    "        model_bert_title.summary()\n",
    "        print('--------------Text Comments MODEL --------------')\n",
    "        model_bert_comments.summary()\n",
    "        print('--------------CONCAT MODEL --------------')\n",
    "        model_concat.summary()\n",
    "        for l in model_concat.layers:\n",
    "            print(l.name, l.trainable)\n",
    "    \n",
    "    \n",
    "    model_concat.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    \n",
    "    history = model_concat.fit(train_seq,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,epochs=EPOCHS, \n",
    "                    validation_data=val_seq,\n",
    "                    validation_steps=STEP_SIZE_VAL, \n",
    "                    callbacks=callbacks_list,\n",
    "                    use_multiprocessing=False,\n",
    "                    verbose=TF_VERBOSE,\n",
    "                    workers=12\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(model_bert_title.layers):\n",
    "#     layer._name = layer._name + '_title' # Consider the _ for the setter\n",
    "        \n",
    "# for i, layer in enumerate(model_bert_title.layers):\n",
    "#     print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained.png'), show_shapes=True, dpi=600, expand_nested=False)\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained_nested.png'), show_shapes=True, dpi=600, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For later model loading\n",
    "# modelFile = '/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_comments_visual_meta_2020-07-19_14:04/weights-improvement-04-0.95.hdf5'\n",
    "\n",
    "# model_concat.load_weights(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeLog(logFilePath, plotTimesPerEpoch(callbacks_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeLog(logFilePath, calcAccConcatModel(model_concat, val_seq, GLOBAL_BATCH_SIZE, 'Val'))\n",
    "writeLog(logFilePath, calcAccConcatModel(model_concat, test_seq, GLOBAL_BATCH_SIZE, 'Test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telegram_send_message(f'-----------------DONE-----------------')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
