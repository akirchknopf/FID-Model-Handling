{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from utils.models.modelUtils import loadMeanFromFile, calcAccConcatModel\n",
    "from final_models import create_image_inceptionv3_model, create_text_model, buildConcatModelCommentsImage\n",
    "\n",
    "from utils.callbacks.MyCallbacks import MyCallbacks\n",
    "from utils.datagenUtils.dual_modal.DataSeqImageTitle import DataSequenceImageTitle\n",
    "from utils.datagenUtils.datagenUtils import checkDataframe\n",
    "\n",
    "from utils.telegramUtils.telegram_bot import telegram_send_message\n",
    "from utils.textUtils.commentsProcessing import FakeDetectionDataCommentsTest, FakeDetectionDataCommentsTrainVal\n",
    "\n",
    "from utils.callbacks.callbackUtils import plotTimesPerEpoch\n",
    "\n",
    "from utils.fileDirUtils.fileDirUtils import createDirIfNotExists\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Batch Size: 96, Epochs: 7, Optimizer: Adam, Learning Rate; 1e-05, Beta_1: 0.9, Beta_2: 0.999, Epsilon: 1e-08, BERT Max sequence length: 128, Image Sizes: (768, 768)\n"
     ]
    }
   ],
   "source": [
    "#Verbose settings:\n",
    "verbose = False\n",
    "TF_VERBOSE = 1 # 1 = Progress bar 2 = one line per epoch only!\n",
    "TF_DETERMINISTIC_OPS = 1 # Makes everything also on GPU deterministic\n",
    "\n",
    "# Classes:\n",
    "NUM_CLASS = 2  # FAKE | NO FAKE\n",
    "\n",
    "# Hyperparameters\n",
    "GLOBAL_BATCH_SIZE = 96\n",
    "EPOCHS = 7\n",
    "\n",
    "# Optimizer parameters:\n",
    "# Adam\n",
    "LEARNING_RATE = 1e-5\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "#optimizers:\n",
    "\n",
    "optimizer = Adam(LEARNING_RATE, BETA_1, BETA_2, EPSILON)\n",
    "\n",
    "# Bert Parameters\n",
    "MAX_SEQUENCE_LENGTH = 128 # from model definition!\n",
    "\n",
    "# Image Model  Parameters\n",
    "IMG_WIDTH = 768\n",
    "IMG_HEIGHT = 768\n",
    "IMG_DEPTH = 3\n",
    "IMG_SIZES = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Retraining options\n",
    "RETRAIN_WHOLE_MODEL = False\n",
    "loadPretrained = True # If true model weights path must be set !\n",
    "\n",
    "# Custom telegram send text \n",
    "CUSTOM_TEXT = f'Batch Size: {GLOBAL_BATCH_SIZE}, Epochs: {EPOCHS}, Optimizer: Adam, Learning Rate; {LEARNING_RATE}, Beta_1: {BETA_1}, Beta_2: {BETA_2}, Epsilon: {EPSILON}, BERT Max sequence length: {MAX_SEQUENCE_LENGTH}, Image Sizes: {IMG_SIZES}'\n",
    "\n",
    "\n",
    "telegram_send_message(f'-----------------START-----------------')\n",
    "print('START')\n",
    "print(CUSTOM_TEXT)\n",
    "telegram_send_message(CUSTOM_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToRootModelDir = '/home/armin/repos/fkd-model-handling/models/best_models'\n",
    "\n",
    "pathToBertModel = os.path.join(pathToRootModelDir, 'single_text_title', 'weights-improvement-02-0.88.hdf5')\n",
    "pathToImageModel = os.path.join(pathToRootModelDir, 'single_visual', 'weights-improvement-02-0.81.hdf5')\n",
    "\n",
    "pathToImagesTrain = '/home/armin/repos/FKD-Dataset/002_images/train/' \n",
    "pathToCSVWithFileNamesAndLabelsTrain = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/train_text_image_meta_label.csv'\n",
    "\n",
    "pathToImagesVal = '/home/armin/repos/FKD-Dataset/002_images/val/' \n",
    "pathToCSVWithFileNamesAndLabelsVal = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/val_text_image_meta_label.csv'\n",
    "\n",
    "pathToImagesTest = '/home/armin/repos/FKD-Dataset/002_images/test/' \n",
    "pathToCSVWithFileNamesAndLabelsTest = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/test_text_image_meta_label.csv'\n",
    "\n",
    "pathToMeans = '/home/armin/repos/FKD-Dataset/010_configs/means_resized.txt'\n",
    "\n",
    "pathToModelWeights = '/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_07:19/weights-improvement-03-0.87.hdf5'\n",
    "\n",
    "checkpointDir = '/home/armin/repos/FKD-Dataset/011_checkpoints'\n",
    "\n",
    "root = '/home/armin/repos/fkd-model-handling/'\n",
    "bert_model_dir = os.path.join(root, 'multi_cased_L-12_H-768_A-12')\n",
    "bert_config_file = os.path.join(bert_model_dir, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(bert_model_dir, \"bert_model.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ettings\n",
    "\n",
    "# Time settings:\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "#Checkpoint settings:\n",
    "checkpoint_name = \"model_concat_comments_image\"\n",
    "\n",
    "checkpointDir = os.path.join(checkpointDir, (checkpoint_name + '_' + current_time))\n",
    "\n",
    "fileName=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filePath = os.path.join(checkpointDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/tensorboard does not exist, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# Callback Handling:\n",
    "tensorboardDir = os.path.join(checkpointDir, 'tensorboard')\n",
    "\n",
    "createDirIfNotExists(tensorboardDir)\n",
    "createDirIfNotExists(checkpointDir)\n",
    "\n",
    "\n",
    "callbacks_list = MyCallbacks(tensorboardDir, filePath, earlyStopping=True).createCheckpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Needed to remove 78 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 28 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 10 elements to make it matching from dataframe\n"
     ]
    }
   ],
   "source": [
    "df_train_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTrain, header=0, sep='\\t')\n",
    "df_test_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTest, header=0, sep='\\t')\n",
    "df_val_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsVal, header=0, sep='\\t')\n",
    "\n",
    "df_train_['2_way_label'] = df_train_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_test_['2_way_label'] = df_test_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_val_['2_way_label'] = df_val_['2_way_label'].apply(lambda x: np.array(x))\n",
    "\n",
    "df_train = df_train_\n",
    "df_test = df_test_\n",
    "df_val = df_val_\n",
    "\n",
    "# df_train = df_train[:8000]\n",
    "# df_test = df_test[:8000]\n",
    "# df_val = df_val[:8000]\n",
    "\n",
    "df_train = checkDataframe(df_train, GLOBAL_BATCH_SIZE)\n",
    "df_val = checkDataframe(df_val, GLOBAL_BATCH_SIZE)\n",
    "df_test = checkDataframe(df_test, GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560544it [33:00, 283.01it/s]\n",
      "58944it [03:33, 276.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 55475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58944it [03:33, 276.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 39424\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_model_dir, \"vocab.txt\"))\n",
    "\n",
    "\n",
    "all_text_data = FakeDetectionDataCommentsTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "all_text_test = FakeDetectionDataCommentsTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "train_x = all_text_data.train_x\n",
    "val_x = all_text_data.val_x\n",
    "test_x = all_text_test.test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansOfDataset = loadMeanFromFile(pathToMeans, verbose)\n",
    "\n",
    "train_seq = DataSequenceImageTitle(df_train, pathToImagesTrain, train_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "test_seq = DataSequenceImageTitle(df_test, pathToImagesTest,  test_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "val_seq = DataSequenceImageTitle(df_val, pathToImagesVal,  val_x, GLOBAL_BATCH_SIZE, IMG_SIZES, meansOfDataset)\n",
    "\n",
    "STEP_SIZE_TRAIN = len(df_train) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_TEST = len(df_test) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_VAL = len(df_val) // GLOBAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "bert shape (None, 128, 768)\n",
      "Epoch 1/7\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8949 - loss: 0.2386\n",
      "Epoch 00001: val_loss improved from inf to 0.27480, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-01-0.88.hdf5\n",
      "5839/5839 [==============================] - 5337s 914ms/step - accuracy: 0.8949 - loss: 0.2386 - val_accuracy: 0.8751 - val_loss: 0.2748 - lr: 1.0000e-05\n",
      "Epoch 2/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8959 - loss: 0.2360\n",
      "Epoch 00002: val_loss improved from 0.27480 to 0.27349, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-02-0.88.hdf5\n",
      "5839/5839 [==============================] - 5370s 920ms/step - accuracy: 0.8959 - loss: 0.2360 - val_accuracy: 0.8766 - val_loss: 0.2735 - lr: 1.0000e-05\n",
      "Epoch 3/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8971 - loss: 0.2336\n",
      "Epoch 00003: val_loss improved from 0.27349 to 0.27164, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-03-0.88.hdf5\n",
      "5839/5839 [==============================] - 5405s 926ms/step - accuracy: 0.8971 - loss: 0.2336 - val_accuracy: 0.8772 - val_loss: 0.2716 - lr: 1.0000e-05\n",
      "Epoch 4/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8980 - loss: 0.2317\n",
      "Epoch 00004: val_loss improved from 0.27164 to 0.27055, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-04-0.88.hdf5\n",
      "5839/5839 [==============================] - 5434s 931ms/step - accuracy: 0.8980 - loss: 0.2317 - val_accuracy: 0.8777 - val_loss: 0.2705 - lr: 1.0000e-05\n",
      "Epoch 5/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8985 - loss: 0.2301\n",
      "Epoch 00005: val_loss improved from 0.27055 to 0.26730, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-05-0.88.hdf5\n",
      "5839/5839 [==============================] - 5426s 929ms/step - accuracy: 0.8985 - loss: 0.2301 - val_accuracy: 0.8784 - val_loss: 0.2673 - lr: 1.0000e-05\n",
      "Epoch 6/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8994 - loss: 0.2289\n",
      "Epoch 00006: val_loss improved from 0.26730 to 0.26593, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_comments_image_2020-07-21_14:19/weights-improvement-06-0.88.hdf5\n",
      "5839/5839 [==============================] - 5486s 940ms/step - accuracy: 0.8994 - loss: 0.2289 - val_accuracy: 0.8795 - val_loss: 0.2659 - lr: 1.0000e-05\n",
      "Epoch 7/7\n",
      "5839/5839 [==============================] - ETA: 0s - accuracy: 0.8996 - loss: 0.2276\n",
      "Epoch 00007: val_loss did not improve from 0.26593\n",
      "5839/5839 [==============================] - 5453s 934ms/step - accuracy: 0.8996 - loss: 0.2276 - val_accuracy: 0.8796 - val_loss: 0.2665 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # Preparing:\n",
    "    model_image, model_image_custom_layer = create_image_inceptionv3_model(NUM_CLASS=NUM_CLASS,isPreTrained=True,pathToInceptionV3ModelWeights=pathToImageModel, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "    model_bert, model_bert_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModel, isTrainable=RETRAIN_WHOLE_MODEL) \n",
    "    \n",
    "    model_concat = buildConcatModelCommentsImage(model_image, model_bert, 2)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------IMAGE MODEL --------------')\n",
    "        model_image.summary()\n",
    "        print('--------------TEXT MODEL --------------')\n",
    "        model_bert.summary()\n",
    "        print('--------------CONCAT MODEL --------------')\n",
    "        model_concat.summary()\n",
    "        for l in model_concat.layers:\n",
    "            print(l.name, l.trainable)\n",
    "            \n",
    "    if loadPretrained:\n",
    "        model_concat.load_weights(pathToModelWeights)\n",
    "    \n",
    "    \n",
    "    model_concat.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    \n",
    "    history = model_concat.fit(train_seq,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,epochs=EPOCHS, \n",
    "                    validation_data=val_seq,\n",
    "                    validation_steps=STEP_SIZE_VAL, \n",
    "                    callbacks=callbacks_list,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=12,\n",
    "                    verbose=TF_VERBOSE\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For later model loading\n",
    "# modelFile = '/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-15_13:16/weights-improvement-02-0.91.hdf5'\n",
    "\n",
    "# model_concat.load_weights(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained.png'), show_shapes=True, dpi=600, expand_nested=False)\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained_nested.png'), show_shapes=True, dpi=600, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took so long to train on one epoch: [90, 90, 90, 91, 91, 92, 91] minutes\n",
      "614/614 [==============================] - 1343s 2s/step\n",
      "Val Accuracy is 0.8795806188925082\n",
      "614/614 [==============================] - 1332s 2s/step\n",
      "Test Accuracy is 0.881243213897937\n"
     ]
    }
   ],
   "source": [
    "plotTimesPerEpoch(callbacks_list)\n",
    "calcAccConcatModel(model_concat, val_seq, GLOBAL_BATCH_SIZE, 'Val')\n",
    "calcAccConcatModel(model_concat, test_seq, GLOBAL_BATCH_SIZE, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "telegram_send_message(f'-----------------DONE-----------------')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
