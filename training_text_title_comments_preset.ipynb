{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from utils.models.modelUtils import loadMeanFromFile, calcAccConcatModel\n",
    "\n",
    "from final_models import create_image_inceptionv3_model, create_text_model, buildConcatModelTitleComments\n",
    "\n",
    "from utils.callbacks.MyCallbacks import MyCallbacks\n",
    "from utils.datagenUtils.dual_modal.DataSeqTitleComments import DataSequenceTitleComments\n",
    "from utils.datagenUtils.datagenUtils import checkDataframe\n",
    "\n",
    "from utils.telegramUtils.telegram_bot import telegram_send_message\n",
    "from utils.textUtils.textpreprocessing import FakeDetectionDataTrainVal, FakeDetectionDataTest\n",
    "from utils.textUtils.commentsProcessing import FakeDetectionDataCommentsTest, FakeDetectionDataCommentsTrainVal\n",
    "\n",
    "from utils.callbacks.callbackUtils import plotTimesPerEpoch\n",
    "\n",
    "from utils.fileDirUtils.fileDirUtils import createDirIfNotExists\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Batch Size: 2048, Epochs: 10, Optimizer: Adam, Learning Rate; 1e-05, Beta_1: 0.9, Beta_2: 0.999, Epsilon: 1e-08, BERT Max sequence length: 128\n"
     ]
    }
   ],
   "source": [
    "#Verbose settings:\n",
    "verbose = False\n",
    "TF_VERBOSE = 1 # 1 = Progress bar 2 = one line per epoch only!\n",
    "TF_DETERMINISTIC_OPS = 1 # Makes everything also on GPU deterministic\n",
    "\n",
    "# Classes:\n",
    "NUM_CLASS = 2  # FAKE | NO FAKE\n",
    "\n",
    "# Hyperparameters\n",
    "GLOBAL_BATCH_SIZE = 2048\n",
    "EPOCHS = 10\n",
    "\n",
    "# Optimizer parameters:\n",
    "# Adam\n",
    "LEARNING_RATE = 1e-5\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "#optimizers:\n",
    "\n",
    "optimizer = Adam(LEARNING_RATE, BETA_1, BETA_2, EPSILON)\n",
    "\n",
    "# Bert Parameters\n",
    "MAX_SEQUENCE_LENGTH = 128 # from model definition!\n",
    "\n",
    "# Training switches\n",
    "RETRAIN_WHOLE_MODEL = False\n",
    "loadPretrained = False # Attention that model weights path is set !\n",
    "\n",
    "# Custom telegram send text \n",
    "CUSTOM_TEXT = f'Batch Size: {GLOBAL_BATCH_SIZE}, Epochs: {EPOCHS}, Optimizer: Adam, Learning Rate; {LEARNING_RATE}, Beta_1: {BETA_1}, Beta_2: {BETA_2}, Epsilon: {EPSILON}, BERT Max sequence length: {MAX_SEQUENCE_LENGTH}'\n",
    "\n",
    "\n",
    "telegram_send_message(f'-----------------START-----------------')\n",
    "print('START')\n",
    "print(CUSTOM_TEXT)\n",
    "telegram_send_message(CUSTOM_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToRootModelDir = '/home/armin/repos/fkd-model-handling/models/best_models'\n",
    "\n",
    "pathToBertModelTitle = os.path.join(pathToRootModelDir, 'single_text_title', 'weights-improvement-02-0.88.hdf5')\n",
    "pathToBertModelComments = os.path.join(pathToRootModelDir, 'single_text_comments', 'weights-improvement-03-0.87.hdf5')\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsTrain = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/train_text_image_meta_label.csv'\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsVal = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/val_text_image_meta_label.csv'\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsTest = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/test_text_image_meta_label.csv'\n",
    "\n",
    "checkpointDir = '/home/armin/repos/FKD-Dataset/011_checkpoints'\n",
    "\n",
    "titleCommentsModelWeightsPath = \"\"\n",
    "\n",
    "root = '/home/armin/repos/fkd-model-handling/'\n",
    "bert_model_dir = os.path.join(root, 'multi_cased_L-12_H-768_A-12')\n",
    "bert_config_file = os.path.join(bert_model_dir, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(bert_model_dir, \"bert_model.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ettings\n",
    "\n",
    "# Time settings:\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "#Checkpoint settings:\n",
    "checkpoint_name = \"model_concat_title_comments\"\n",
    "\n",
    "checkpointDir = os.path.join(checkpointDir, (checkpoint_name + '_' + current_time))\n",
    "\n",
    "fileName=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filePath = os.path.join(checkpointDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/tensorboard does not exist, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# Callback Handling:\n",
    "tensorboardDir = os.path.join(checkpointDir, 'tensorboard')\n",
    "\n",
    "createDirIfNotExists(tensorboardDir)\n",
    "createDirIfNotExists(checkpointDir)\n",
    "\n",
    "\n",
    "callbacks_list = MyCallbacks(tensorboardDir, filePath, earlyStopping=True).createCheckpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Needed to remove 1518 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 1628 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 1610 elements to make it matching from dataframe\n"
     ]
    }
   ],
   "source": [
    "df_train_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTrain, header=0, sep='\\t')\n",
    "df_test_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTest, header=0, sep='\\t')\n",
    "df_val_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsVal, header=0, sep='\\t')\n",
    "\n",
    "df_train_['2_way_label'] = df_train_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_test_['2_way_label'] = df_test_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_val_['2_way_label'] = df_val_['2_way_label'].apply(lambda x: np.array(x))\n",
    "\n",
    "df_train = df_train_\n",
    "df_test = df_test_\n",
    "df_val = df_val_\n",
    "\n",
    "# df_train = df_train[:15000]\n",
    "# df_test = df_test[:15000]\n",
    "# df_val = df_val[:15000]\n",
    "\n",
    "df_train = checkDataframe(df_train, GLOBAL_BATCH_SIZE)\n",
    "df_val = checkDataframe(df_val, GLOBAL_BATCH_SIZE)\n",
    "df_test = checkDataframe(df_test, GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "559104it [02:06, 4425.89it/s]\n",
      "57344it [00:13, 4374.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57344it [00:12, 4500.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "559104it [31:36, 294.88it/s]\n",
      "57344it [03:22, 282.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 55475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57344it [03:20, 286.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 39424\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_model_dir, \"vocab.txt\"))\n",
    "\n",
    "all_title_data = FakeDetectionDataTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "all_title_test = FakeDetectionDataTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "comments_data_train = FakeDetectionDataCommentsTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "comments_data_test = FakeDetectionDataCommentsTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "train_title_x = all_title_data.train_x\n",
    "train_title_val_x = all_title_data.val_x\n",
    "test_title_x = all_title_test.test_x\n",
    "\n",
    "comments_train_x = comments_data_train.train_x\n",
    "comments_val_x = comments_data_train.val_x\n",
    "comments_test_x = comments_data_test.test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = DataSequenceTitleComments(df_train, train_title_x, comments_train_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "test_seq = DataSequenceTitleComments(df_test,  test_title_x, comments_test_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "val_seq = DataSequenceTitleComments(df_val,  train_title_val_x, comments_val_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "STEP_SIZE_TRAIN = len(df_train) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_TEST = len(df_test) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_VAL = len(df_val) // GLOBAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buildConcatModelTitleComments(model_bert_title, model_bert_comments, NUM_CLASS):\n",
    "#     model_bert_title = model_bert_title.get_layer('bert_output_layer_768').output\n",
    "#     model_bert_comments = model_bert.get_layer('bert_output_layer_768').output\n",
    "    \n",
    "#     # Build new models, without softmax\n",
    "#     model_bert_title = Model(model_bert_title.inputs, model_bert_title)    \n",
    "#     model_bert_comments = Model(model_bert_comments.inputs, model_bert_comments)\n",
    "    \n",
    "#     concatenate = Concatenate()([model_bert_title.output, model_bert_comments.output]) # Fusion\n",
    "#     x = Dense(512, activation = 'relu')(concatenate)\n",
    "#     x = Dropout(0.4)(x)\n",
    "#     output = Dense(NUM_CLASS, activation='softmax', name='final_multimodal_output')(x)\n",
    "#     model_concat = Model([model_bert_title.input, model_bert_comments.input], output)\n",
    "#     return model_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "bert shape (None, 128, 768)\n",
      "bert shape (None, 128, 768)\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.7137 - loss: 0.5517INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42192, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-01-0.81.hdf5\n",
      "273/273 [==============================] - 2348s 9s/step - accuracy: 0.7137 - loss: 0.5517 - val_accuracy: 0.8141 - val_loss: 0.4219 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8067 - loss: 0.4352\n",
      "Epoch 00002: val_loss improved from 0.42192 to 0.39349, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-02-0.83.hdf5\n",
      "273/273 [==============================] - 2354s 9s/step - accuracy: 0.8067 - loss: 0.4352 - val_accuracy: 0.8253 - val_loss: 0.3935 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8183 - loss: 0.4106\n",
      "Epoch 00003: val_loss improved from 0.39349 to 0.37757, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-03-0.83.hdf5\n",
      "273/273 [==============================] - 2351s 9s/step - accuracy: 0.8183 - loss: 0.4106 - val_accuracy: 0.8326 - val_loss: 0.3776 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8258 - loss: 0.3942\n",
      "Epoch 00004: val_loss improved from 0.37757 to 0.36520, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-04-0.84.hdf5\n",
      "273/273 [==============================] - 2351s 9s/step - accuracy: 0.8258 - loss: 0.3942 - val_accuracy: 0.8377 - val_loss: 0.3652 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8316 - loss: 0.3825\n",
      "Epoch 00005: val_loss improved from 0.36520 to 0.35475, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-05-0.84.hdf5\n",
      "273/273 [==============================] - 2355s 9s/step - accuracy: 0.8316 - loss: 0.3825 - val_accuracy: 0.8444 - val_loss: 0.3548 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8367 - loss: 0.3726\n",
      "Epoch 00006: val_loss improved from 0.35475 to 0.34617, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-06-0.85.hdf5\n",
      "273/273 [==============================] - 2350s 9s/step - accuracy: 0.8367 - loss: 0.3726 - val_accuracy: 0.8482 - val_loss: 0.3462 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8400 - loss: 0.3649\n",
      "Epoch 00007: val_loss improved from 0.34617 to 0.33974, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-07-0.85.hdf5\n",
      "273/273 [==============================] - 2352s 9s/step - accuracy: 0.8400 - loss: 0.3649 - val_accuracy: 0.8503 - val_loss: 0.3397 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8432 - loss: 0.3589\n",
      "Epoch 00008: val_loss improved from 0.33974 to 0.33353, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-08-0.85.hdf5\n",
      "273/273 [==============================] - 2353s 9s/step - accuracy: 0.8432 - loss: 0.3589 - val_accuracy: 0.8531 - val_loss: 0.3335 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8456 - loss: 0.3531\n",
      "Epoch 00009: val_loss improved from 0.33353 to 0.32977, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-09-0.85.hdf5\n",
      "273/273 [==============================] - 2353s 9s/step - accuracy: 0.8456 - loss: 0.3531 - val_accuracy: 0.8548 - val_loss: 0.3298 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "273/273 [==============================] - ETA: 0s - accuracy: 0.8469 - loss: 0.3494\n",
      "Epoch 00010: val_loss improved from 0.32977 to 0.32577, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-16_09:06/weights-improvement-10-0.86.hdf5\n",
      "273/273 [==============================] - 2355s 9s/step - accuracy: 0.8469 - loss: 0.3494 - val_accuracy: 0.8559 - val_loss: 0.3258 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # Preparing:\n",
    "    model_bert_title, model_bert_title_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModelTitle, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "    model_bert_comments, model_bert_comments_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModelComments, isTrainable=RETRAIN_WHOLE_MODEL) \n",
    "    \n",
    "    # Handling same layer name error:\n",
    "    for i, layer in enumerate(model_bert_title.layers):\n",
    "        layer._name = layer._name + '_title' # Consider the _ for the setter\n",
    "    \n",
    "    model_concat = buildConcatModelTitleComments(model_bert_title, model_bert_comments, 2)\n",
    "    \n",
    "    if loadPretrained:\n",
    "        model_concat.load_weights(titleCommentsModelWeightsPath)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------Text Title MODEL --------------')\n",
    "        model_bert_title.summary()\n",
    "        print('--------------Text Comments MODEL --------------')\n",
    "        model_bert_comments.summary()\n",
    "        print('--------------CONCAT MODEL --------------')\n",
    "        model_concat.summary()\n",
    "        for l in model_concat.layers:\n",
    "            print(l.name, l.trainable)\n",
    "    \n",
    "    \n",
    "    model_concat.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    \n",
    "    history = model_concat.fit(train_seq,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,epochs=EPOCHS, \n",
    "                    validation_data=val_seq,\n",
    "                    validation_steps=STEP_SIZE_VAL, \n",
    "                    callbacks=callbacks_list,\n",
    "                    use_multiprocessing=False,\n",
    "                    verbose=TF_VERBOSE,\n",
    "                    workers=12\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(model_bert_title.layers):\n",
    "#     layer._name = layer._name + '_title' # Consider the _ for the setter\n",
    "        \n",
    "# for i, layer in enumerate(model_bert_title.layers):\n",
    "#     print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained.png'), show_shapes=True, dpi=600, expand_nested=False)\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained_nested.png'), show_shapes=True, dpi=600, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For later model loading\n",
    "# modelFile = '/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-15_13:16/weights-improvement-02-0.91.hdf5'\n",
    "\n",
    "# model_concat.load_weights(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took so long to train on one epoch: [41, 39, 39, 39, 39, 39, 39, 39, 39, 39] minutes\n"
     ]
    }
   ],
   "source": [
    "plotTimesPerEpoch(callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 189s 7s/step\n",
      "Val Accuracy is 0.8559047154017857\n",
      "28/28 [==============================] - 189s 7s/step\n",
      "Test Accuracy is 0.8571602957589286\n"
     ]
    }
   ],
   "source": [
    "calcAccConcatModel(model_concat, val_seq, GLOBAL_BATCH_SIZE, 'Val')\n",
    "calcAccConcatModel(model_concat, test_seq, GLOBAL_BATCH_SIZE, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "telegram_send_message(f'-----------------DONE-----------------')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
