{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from utils.models.modelUtils import loadMeanFromFile, calcAccConcatModel\n",
    "\n",
    "from final_models import create_text_model, create_model_meta, buildConcatModelTitleMeta\n",
    "\n",
    "from utils.callbacks.MyCallbacks import MyCallbacks\n",
    "from utils.datagenUtils.dual_modal.DataSeqTitleMeta import DataSequenceTitleMeta\n",
    "from utils.datagenUtils.datagenUtils import checkDataframe\n",
    "\n",
    "from utils.telegramUtils.telegram_bot import telegram_send_message\n",
    "from utils.textUtils.textpreprocessing import FakeDetectionDataTrainVal, FakeDetectionDataTest\n",
    "from utils.textUtils.commentsProcessing import FakeDetectionDataCommentsTest, FakeDetectionDataCommentsTrainVal\n",
    "\n",
    "from utils.callbacks.callbackUtils import plotTimesPerEpoch\n",
    "\n",
    "from utils.fileDirUtils.fileDirUtils import createDirIfNotExists\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Batch Size: 4096, Epochs: 10, Optimizer: Adam, Learning Rate; 1e-05, Beta_1: 0.9, Beta_2: 0.999, Epsilon: 1e-08, BERT Max sequence length: 128\n"
     ]
    }
   ],
   "source": [
    "#Verbose settings:\n",
    "verbose = False\n",
    "TF_VERBOSE = 1 # 1 = Progress bar 2 = one line per epoch only!\n",
    "TF_DETERMINISTIC_OPS = 1 # Makes everything also on GPU deterministic\n",
    "\n",
    "# Classes:\n",
    "NUM_CLASS = 2  # FAKE | NO FAKE\n",
    "\n",
    "# Hyperparameters\n",
    "GLOBAL_BATCH_SIZE = 4096\n",
    "EPOCHS = 15\n",
    "\n",
    "# Optimizer parameters:\n",
    "# Adam\n",
    "LEARNING_RATE = 1e-5\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "#optimizers:\n",
    "\n",
    "optimizer = Adam(LEARNING_RATE, BETA_1, BETA_2, EPSILON)\n",
    "\n",
    "# Bert Parameters\n",
    "MAX_SEQUENCE_LENGTH = 128 # from model definition!\n",
    "\n",
    "# Training switches\n",
    "RETRAIN_WHOLE_MODEL = False\n",
    "loadPretrained = False # Attention that model weights path is set !\n",
    "\n",
    "# Custom telegram send text \n",
    "CUSTOM_TEXT = f'Batch Size: {GLOBAL_BATCH_SIZE}, Epochs: {EPOCHS}, Optimizer: Adam, Learning Rate; {LEARNING_RATE}, Beta_1: {BETA_1}, Beta_2: {BETA_2}, Epsilon: {EPSILON}, BERT Max sequence length: {MAX_SEQUENCE_LENGTH}'\n",
    "\n",
    "\n",
    "telegram_send_message(f'-----------------START-----------------')\n",
    "print('START')\n",
    "print(CUSTOM_TEXT)\n",
    "telegram_send_message(CUSTOM_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToRootModelDir = '/home/armin/repos/fkd-model-handling/models/best_models'\n",
    "\n",
    "pathToBertModelTitle = os.path.join(pathToRootModelDir, 'single_text_title', 'weights-improvement-02-0.88.hdf5')\n",
    "pathToMetaModel = os.path.join(pathToRootModelDir, 'single_meta', 'weights-improvement-100-0.62.hdf5')\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsTrain = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/train_text_image_meta_label.csv'\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsVal = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/val_text_image_meta_label.csv'\n",
    "\n",
    "pathToCSVWithFileNamesAndLabelsTest = '/home/armin/repos/FKD-Dataset/008_text_image_meta_label/test_text_image_meta_label.csv'\n",
    "\n",
    "checkpointDir = '/home/armin/repos/FKD-Dataset/011_checkpoints'\n",
    "\n",
    "titleMetaModelWeightsPath = \"\"\n",
    "\n",
    "root = '/home/armin/repos/fkd-model-handling/'\n",
    "bert_model_dir = os.path.join(root, 'multi_cased_L-12_H-768_A-12')\n",
    "bert_config_file = os.path.join(bert_model_dir, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(bert_model_dir, \"bert_model.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ettings\n",
    "\n",
    "# Time settings:\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "#Checkpoint settings:\n",
    "checkpoint_name = \"model_concat_title_meta\"\n",
    "\n",
    "checkpointDir = os.path.join(checkpointDir, (checkpoint_name + '_' + current_time))\n",
    "\n",
    "fileName=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filePath = os.path.join(checkpointDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/tensorboard does not exist, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# Callback Handling:\n",
    "tensorboardDir = os.path.join(checkpointDir, 'tensorboard')\n",
    "\n",
    "createDirIfNotExists(tensorboardDir)\n",
    "createDirIfNotExists(checkpointDir)\n",
    "\n",
    "\n",
    "callbacks_list = MyCallbacks(tensorboardDir, filePath, earlyStopping=True).createCheckpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Needed to remove 3566 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 1628 elements to make it matching from dataframe\n",
      "Warning: Needed to remove 1610 elements to make it matching from dataframe\n"
     ]
    }
   ],
   "source": [
    "df_train_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTrain, header=0, sep='\\t')\n",
    "df_test_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsTest, header=0, sep='\\t')\n",
    "df_val_ = pd.read_csv(pathToCSVWithFileNamesAndLabelsVal, header=0, sep='\\t')\n",
    "\n",
    "df_train_['2_way_label'] = df_train_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_test_['2_way_label'] = df_test_['2_way_label'].apply(lambda x: np.array(x))\n",
    "df_val_['2_way_label'] = df_val_['2_way_label'].apply(lambda x: np.array(x))\n",
    "\n",
    "df_train = df_train_\n",
    "df_test = df_test_\n",
    "df_val = df_val_\n",
    "\n",
    "# df_train = df_train[:15000]\n",
    "# df_test = df_test[:15000]\n",
    "# df_val = df_val[:15000]\n",
    "\n",
    "df_train = checkDataframe(df_train, GLOBAL_BATCH_SIZE)\n",
    "df_val = checkDataframe(df_val, GLOBAL_BATCH_SIZE)\n",
    "df_test = checkDataframe(df_test, GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "557056it [02:03, 4507.04it/s]\n",
      "57344it [00:12, 4519.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57344it [00:12, 4502.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 128\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_model_dir, \"vocab.txt\"))\n",
    "\n",
    "all_title_data = FakeDetectionDataTrainVal(df_train, df_val, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "all_title_test = FakeDetectionDataTest(df_test, tokenizer, [0,1], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "train_title_x = all_title_data.train_x\n",
    "train_title_val_x = all_title_data.val_x\n",
    "test_title_x = all_title_test.test_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = DataSequenceTitleMeta(df_train, train_title_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "test_seq = DataSequenceTitleMeta(df_test, test_title_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "val_seq = DataSequenceTitleMeta(df_val, train_title_val_x, GLOBAL_BATCH_SIZE)\n",
    "\n",
    "STEP_SIZE_TRAIN = len(df_train) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_TEST = len(df_test) // GLOBAL_BATCH_SIZE\n",
    "STEP_SIZE_VAL = len(df_val) // GLOBAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Concatenate, Dense, Dropout, GlobalAveragePooling2D, Input, Lambda\n",
    "# def buildConcatModelTitleMeta(model_bert_title, model_meta, NUM_CLASS):\n",
    "#     model_bert_title_output = model_bert_title.get_layer('bert_output_layer_768').output\n",
    "#     model_meta_output = model_meta.get_layer('final_output').output\n",
    "    \n",
    "#     # Build new models, without softmax\n",
    "#     model_bert_title = Model(model_bert_title.inputs, model_bert_title_output)    \n",
    "#     model_meta = Model(model_meta.inputs, model_meta_output)\n",
    "    \n",
    "#     x = Dense(128, activation = 'relu')(model_bert_title.output)\n",
    "#     x = Dense (32, activation = 'relu')(x)\n",
    "#     x = Dense (6, activation = 'relu')(x)\n",
    "#     concatenate = Concatenate()([model_bert_title.output, x]) # Fusion\n",
    "#     x = Dense(12, activation = 'relu')(concatenate)\n",
    "#     x = Dropout(0.4)(x)\n",
    "#     output = Dense(NUM_CLASS, activation='softmax', name='final_multimodal_output')(x)\n",
    "#     model_concat = Model([model_bert_title.input, model_meta.input], output)\n",
    "#     return model_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "bert shape (None, 128, 768)\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5701INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54456, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-01-0.78.hdf5\n",
      "136/136 [==============================] - 1175s 9s/step - loss: 0.6960 - accuracy: 0.5701 - val_loss: 0.5446 - val_accuracy: 0.7827 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.7330\n",
      "Epoch 00002: val_loss improved from 0.54456 to 0.42779, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-02-0.88.hdf5\n",
      "136/136 [==============================] - 1182s 9s/step - loss: 0.5268 - accuracy: 0.7330 - val_loss: 0.4278 - val_accuracy: 0.8753 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.7720\n",
      "Epoch 00003: val_loss improved from 0.42779 to 0.34344, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-03-0.88.hdf5\n",
      "136/136 [==============================] - 1181s 9s/step - loss: 0.4374 - accuracy: 0.7720 - val_loss: 0.3434 - val_accuracy: 0.8808 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8053\n",
      "Epoch 00004: val_loss improved from 0.34344 to 0.30422, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-04-0.88.hdf5\n",
      "136/136 [==============================] - 1182s 9s/step - loss: 0.3824 - accuracy: 0.8053 - val_loss: 0.3042 - val_accuracy: 0.8811 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.8332\n",
      "Epoch 00005: val_loss improved from 0.30422 to 0.29088, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-05-0.88.hdf5\n",
      "136/136 [==============================] - 1181s 9s/step - loss: 0.3525 - accuracy: 0.8332 - val_loss: 0.2909 - val_accuracy: 0.8813 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.8405\n",
      "Epoch 00006: val_loss improved from 0.29088 to 0.28664, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-06-0.88.hdf5\n",
      "136/136 [==============================] - 1182s 9s/step - loss: 0.3373 - accuracy: 0.8405 - val_loss: 0.2866 - val_accuracy: 0.8811 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8431\n",
      "Epoch 00007: val_loss improved from 0.28664 to 0.28528, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-07-0.88.hdf5\n",
      "136/136 [==============================] - 1181s 9s/step - loss: 0.3282 - accuracy: 0.8431 - val_loss: 0.2853 - val_accuracy: 0.8814 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8453\n",
      "Epoch 00008: val_loss improved from 0.28528 to 0.28456, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-08-0.88.hdf5\n",
      "136/136 [==============================] - 1182s 9s/step - loss: 0.3229 - accuracy: 0.8453 - val_loss: 0.2846 - val_accuracy: 0.8812 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8468\n",
      "Epoch 00009: val_loss improved from 0.28456 to 0.28423, saving model to /home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_meta_2020-07-17_21:15/weights-improvement-09-0.88.hdf5\n",
      "136/136 [==============================] - 1182s 9s/step - loss: 0.3200 - accuracy: 0.8468 - val_loss: 0.2842 - val_accuracy: 0.8812 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.8483\n",
      "Epoch 00010: val_loss did not improve from 0.28423\n",
      "136/136 [==============================] - 1173s 9s/step - loss: 0.3169 - accuracy: 0.8483 - val_loss: 0.2844 - val_accuracy: 0.8814 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # Preparing:\n",
    "    model_bert_title, model_bert_title_custom_layer = create_text_model(max_seq_len=MAX_SEQUENCE_LENGTH, bert_ckpt_file=bert_ckpt_file, bert_config_file= bert_config_file,NUM_CLASS=NUM_CLASS, overwriteLayerAndEmbeddingSize=False, isPreTrained=True,  pathToBertModelWeights=pathToBertModelTitle, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "    model_meta, model_meta_custom_layer = create_model_meta(NUM_CLASS, 2, True, pathToMetaModel, isTrainable=RETRAIN_WHOLE_MODEL)\n",
    "    \n",
    "    model_concat = buildConcatModelTitleMeta(model_bert_title, model_meta, 2)\n",
    "    \n",
    "    if loadPretrained:\n",
    "        model_concat.load_weights(ModelTitleMetaWeightsPath)\n",
    "\n",
    "    if verbose:\n",
    "        print('--------------Text Title MODEL --------------')\n",
    "        model_bert_title.summary()\n",
    "        print('-------------- Meta MODEL --------------')\n",
    "        model_meta.summary()\n",
    "        print('--------------CONCAT MODEL --------------')\n",
    "        model_concat.summary()\n",
    "        for l in model_concat.layers:\n",
    "            print(l.name, l.trainable)\n",
    "    \n",
    "    \n",
    "    model_concat.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    \n",
    "    history = model_concat.fit(train_seq,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,epochs=EPOCHS, \n",
    "                    validation_data=val_seq,\n",
    "                    validation_steps=STEP_SIZE_VAL, \n",
    "                    callbacks=callbacks_list,\n",
    "                    use_multiprocessing=False,\n",
    "                    verbose=TF_VERBOSE,\n",
    "                    workers=12\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained.png'), show_shapes=True, dpi=600, expand_nested=False)\n",
    "plot_model(model_concat, to_file=os.path.join(checkpointDir,'multiple_inputs_image_text_meta_pretrained_nested.png'), show_shapes=True, dpi=600, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For later model loading\n",
    "# modelFile = '/home/armin/repos/FKD-Dataset/011_checkpoints/model_concat_title_image_2020-07-15_13:16/weights-improvement-02-0.91.hdf5'\n",
    "\n",
    "# model_concat.load_weights(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took so long to train on one epoch: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20] minutes\n"
     ]
    }
   ],
   "source": [
    "plotTimesPerEpoch(callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 92s 7s/step\n",
      "Val Accuracy is 0.8813825334821429\n",
      "14/14 [==============================] - 92s 7s/step\n",
      "Test Accuracy is 0.881591796875\n"
     ]
    }
   ],
   "source": [
    "calcAccConcatModel(model_concat, val_seq, GLOBAL_BATCH_SIZE, 'Val')\n",
    "calcAccConcatModel(model_concat, test_seq, GLOBAL_BATCH_SIZE, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "telegram_send_message(f'-----------------DONE-----------------')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
